{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests For Natural Language Processing\n",
    "\n",
    "This NoteBook is used to test the Spark-NLP for processing text\n",
    "\n",
    "## Initialize Spark-NLP\n",
    "To initialize Spark-NLP it is neccesary to import the library and use the method start().\n",
    "\n",
    "For Spark-NLP to work it is also neccessary to install the libary pyspark using pip is not installed using the requirements.txt file.\n",
    "\n",
    "```sh\n",
    "> pip install pyspark==3.3.1\n",
    "``` \n",
    "\n",
    "It is also neccesary to install Java. For example in Ubuntu:\n",
    "\n",
    "```sh\n",
    "> sudo apt install openjdk-17-jdk-headless\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check pretained models available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Pipeline\n",
    "The following code shows hot to use a pretrained pipeline.\n",
    "This example has been obtained from sparknlp.org site. \n",
    "\n",
    "In this case a pretained model is used to perform a series of NLP task in a given text, generally before processing the text.\n",
    "For example we would preproccess - annotate - the text before trying to classificate or translate it with a model - or before using it for trainning a model -. Ofcourse, more steps would be needed in some cases so the input of the model can proccess the information, like converting the \"words\"/tokens to a numeric value - this is called [word embedding](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) -.\n",
    "\n",
    "Some of these task are:\n",
    "\n",
    "\n",
    "- Token: divides the sentences in tokens, generally it correspond to each word separated by a space, minus the puntuation. In words like \"don't\", the correct way in which the token is divided is \"do\" and \"n't\", so it is easier for the algorithms to undertand that is a negative no.\n",
    "\n",
    "- lemmatization: it simplifies the tokens to get common forms from inflections. For example, orginze, organizes and organizing will be converted to orginize.\n",
    "\n",
    "- pos: determines the type of token, and gives a tag to each type. For example: JJ adjetive, NNP proper name singular, VBP Verb, non-3rd person singular present... [TAGS](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html)\n",
    "\n",
    "- Stemming: is similar to lemmatization, but instead of looking for a base form of a work, it will just chop the end of some words. For example, if will have Finally and Final, the common used token will be Fina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:21:45 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "24/09/30 00:21:45 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approximate size to download 9 MB\n",
      "Download done! Loading the resource.\n",
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "document: [\"We are very happy about SparkNLP. But We don't know how to use it yet.\"]\n",
      "spell: [\"We\", \"are\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "pos: [\"PRP\", \"VBP\", \"RB\", \"JJ\", \"IN\", \"NNP\", \".\", \"CC\", \"PRP\", \"VBP\", \"VB\", \"WRB\", \"TO\", \"VB\", \"PRP\", \"RB\", \".\"]\n",
      "lemmas: [\"We\", \"be\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "token: [\"We\", \"are\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "stems: [\"we\", \"ar\", \"veri\", \"happi\", \"about\", \"sparknlp\", \".\", \"but\", \"we\", \"don't\", \"know\", \"how\", \"to\", \"us\", \"it\", \"yet\", \".\"]\n",
      "sentence: [\"We are very happy about SparkNLP.\", \"But We don't know how to use it yet.\"]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import json\n",
    "\n",
    "explain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n",
    "annotations = explain_document_pipeline.annotate(\"We are very happy about SparkNLP. But We don't know how to use it yet.\")\n",
    "\n",
    "for key, array in annotations.items():\n",
    "    formatted_array = ', '.join(json.dumps(item) for item in array)\n",
    "    print(f\"{key}: [{formatted_array}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text Sentiment\n",
    "The following code shows how to use a pretained spark-nlp model to classify text.\n",
    "\n",
    "In this case the classifier will check for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "analyze_sentiment download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:22:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx size to download 4,8 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:22:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "24/09/30 00:22:02 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_sentiment download started this may take some time.\n",
      "Approximate size to download 4,8 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "|id |text               |result    |\n",
      "+---+-------------------+----------+\n",
      "|1  |I hate you         |[negative]|\n",
      "|2  |I love this product|[positive]|\n",
      "|3  |You are an idiot   |[positive]|\n",
      "|4  |This is amazing    |[positive]|\n",
      "+---+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# Example using a pre-trained sentiment analysis pipeline\n",
    "# Replace with a relevant pipeline if available\n",
    "pipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"I love this product\"),\n",
    "    (3, \"You are an idiot\"),\n",
    "    (4, \"This is amazing\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Apply pipeline\n",
    "result = pipeline.transform(data)\n",
    "\n",
    "result.select(\"id\", \"text\", \"sentiment.result\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text Hate Speech with pretrained models\n",
    "The following code shows how to use a pretained spark-nlp model to classify text, while a like shows the line fit, this does not train the modle.\n",
    "\n",
    "In this case the classifier will check for hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_sequence_classifier_dehatebert_mono download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:26:32 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 599 MB\n",
      "[OK!]\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|id |text                                                                                                                                            |class                                                                                          |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|1  |I hate you                                                                                                                                      |[{category, 0, 9, NON_HATE, {sentence -> 0, NON_HATE -> 0.58358485, HATE -> 0.41641515}, []}]  |\n",
      "|2  |You are an amazing person.                                                                                                                      |[{category, 0, 25, NON_HATE, {sentence -> 0, NON_HATE -> 0.97737086, HATE -> 0.022629123}, []}]|\n",
      "|3  |This is terrible.                                                                                                                               |[{category, 0, 16, NON_HATE, {sentence -> 0, NON_HATE -> 0.9723487, HATE -> 0.02765133}, []}]  |\n",
      "|4  |What a wonderful day!                                                                                                                           |[{category, 0, 20, NON_HATE, {sentence -> 0, NON_HATE -> 0.9770796, HATE -> 0.02292045}, []}]  |\n",
      "|5  |You are a moron.                                                                                                                                |[{category, 0, 15, NON_HATE, {sentence -> 0, NON_HATE -> 0.8094599, HATE -> 0.19054009}, []}]  |\n",
      "|6  |** are cool                                                                                                                                  |[{category, 0, 13, NON_HATE, {sentence -> 0, NON_HATE -> 0.9745119, HATE -> 0.025488053}, []}] |\n",
      "|7  |** are not cool                                                                                                                              |[{category, 0, 17, NON_HATE, {sentence -> 0, NON_HATE -> 0.9746483, HATE -> 0.025351731}, []}] |\n",
      "|8  |RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corrâ€¦|[{category, 0, 139, HATE, {sentence -> 0, NON_HATE -> 0.09545476, HATE -> 0.90454525}, []}]    |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, BertForSequenceClassification\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 0. Configure the step for document assembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol('text') \\\n",
    ".setOutputCol('document')\n",
    "\n",
    "# 1. Configure the step that will get tokens from text\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "\n",
    "# 2. Configure the step that will classify the text\n",
    "sequenceClassifier = BertForSequenceClassification \\\n",
    "    .pretrained('bert_sequence_classifier_dehatebert_mono', 'en') \\\n",
    "    .setInputCols(['token', 'document']) \\\n",
    "    .setOutputCol('class') \\\n",
    "    .setCaseSensitive(True) \\\n",
    "    .setMaxSentenceLength(512)\n",
    "\n",
    "# 3. Configure the step that will obtain the result from the proccess\n",
    "#finisher = Finisher() \\\n",
    "#    .setInputCols([\"class\"]) \\\n",
    "#    .setOutputCols([\"prediction\"]) \\\n",
    "#    .setCleanAnnotations(False)\n",
    "\n",
    "# 4. Organize the steps\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    sequenceClassifier\n",
    "    #finisher\n",
    "])\n",
    "\n",
    "# 5. Classify sample data\n",
    "# Sample data\n",
    "example = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"You are an amazing person.\"),\n",
    "    (3, \"This is terrible.\"),\n",
    "    (4, \"What a wonderful day!\"),\n",
    "    (5, \"You are a moron.\"), \n",
    "    (6,\"** are cool\"), \n",
    "    (7,\"** are not cool\"),\n",
    "    (8,\"RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corrâ€¦\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# If we want to modify the model\n",
    "#model = pipeline.fit(example)\n",
    "\n",
    "result = pipeline.fit(example).transform(example)\n",
    "result.select(\"id\", \"text\", \"class\").show(truncate=False)\n",
    "#result.select(\"id\", \"text\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_classifier_bert_base_uncased_hatexplain download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:27:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 390,4 MB\n",
      "[OK!]\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                             |class                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|I hate you                                                                                                                                       |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.13802324, normal -> 0.5470316, offensive -> 0.3149452}, []}]         |\n",
      "|You are an amazing person.                                                                                                                       |[{category, 0, 25, normal, {sentence -> 0, hate speech -> 0.060175754, normal -> 0.64119154, offensive -> 0.29863268}, []}]     |\n",
      "|This is terrible.                                                                                                                                |[{category, 0, 16, normal, {sentence -> 0, hate speech -> 0.07880838, normal -> 0.62920123, offensive -> 0.2919904}, []}]       |\n",
      "|What a wonderful day!                                                                                                                            |[{category, 0, 20, normal, {sentence -> 0, hate speech -> 0.06310587, normal -> 0.6823274, offensive -> 0.25456673}, []}]       |\n",
      "|You are a moron.                                                                                                                                 |[{category, 0, 15, offensive, {sentence -> 0, hate speech -> 0.051471725, normal -> 0.3786545, offensive -> 0.56987375}, []}]   |\n",
      "|** are cool                                                                                                                                   |[{category, 0, 13, offensive, {sentence -> 0, hate speech -> 0.12458121, normal -> 0.36013916, offensive -> 0.5152796}, []}]    |\n",
      "|RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corrâ€¦|[{category, 0, 140, hate speech, {sentence -> 0, hate speech -> 0.76267284, normal -> 0.09264287, offensive -> 0.14468427}, []}]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|text              |class                                                                                                                       |\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|I hate you        |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.13802324, normal -> 0.5470316, offensive -> 0.3149452}, []}]     |\n",
      "|I love you        |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.08458458, normal -> 0.66519564, offensive -> 0.25021976}, []}]   |\n",
      "|** are cool    |[{category, 0, 13, offensive, {sentence -> 0, hate speech -> 0.12458121, normal -> 0.36013916, offensive -> 0.5152796}, []}]|\n",
      "|** are not cool|[{category, 0, 17, offensive, {sentence -> 0, hate speech -> 0.13478369, normal -> 0.31091493, offensive -> 0.5543014}, []}]|\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, BertForSequenceClassification\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "seq_classifier = BertForSequenceClassification.pretrained(\"bert_classifier_bert_base_uncased_hatexplain\",\"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"class\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler, tokenizer, seq_classifier])\n",
    "\n",
    "example = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"You are an amazing person.\"),\n",
    "    (3, \"This is terrible.\"),\n",
    "    (4, \"What a wonderful day!\"),\n",
    "    (5, \"You are a moron.\"), \n",
    "    (6,\"** are cool\"), \n",
    "    (7,\"RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corrâ€¦\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "result = pipeline.fit(example).transform(example)\n",
    "result.select(\"text\", \"class\").show(truncate=False)\n",
    "\n",
    "data = spark.createDataFrame(\n",
    "    [(1,\"I hate you\"), (2,\"I love you\"), (3,\"** are cool\"), (4,\"** are not cool\")], \n",
    "    [\"id\",\"text\"]\n",
    ")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.select(\"text\", \"class\").show(truncate=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
